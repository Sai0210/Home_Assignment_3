## Home_Assignment_3
## Student Name : Sai krishna Edara
## Student Id : 700769262
## Question 1
### Code Description
#### Imports Libraries:
Imports necessary libraries: numpy for numerical operations, matplotlib.pyplot for plotting, and tensorflow and its Keras API for building and training the neural network.
#### Loads MNIST Dataset:
Loads the MNIST dataset using tensorflow.keras.datasets.mnist.load_data(). The dataset is split into training and testing sets.
#### Preprocesses Data:
Converts the pixel values to floating-point numbers and normalizes them to the range [0, 1] by dividing by 255.0.
Flattens the 28x28 pixel images into vectors of 784 elements using reshape().
#### Defines the Autoencoder Model:
##### Encoder
An Input layer is defined with a shape of (784,), representing the flattened input image.
A Dense layer with latent_dim (set to 32) units and ReLU activation is used to encode the input into the latent space.
##### Decoder
A Dense layer with 784 units (the original image dimension) and sigmoid activation is used to decode the latent representation back to the image space. Sigmoid activation ensures the output pixel values are between 0 and 1.
##### Autoencoder Model:
A Model is created that takes the input_img and outputs the decoded image.
##### Compilation
The autoencoder model is compiled using the adam optimizer and binary_crossentropy loss function, which is suitable for pixel-wise binary classification (whether a pixel is "on" or "off").
#### Trains the Model:

The fit() method trains the autoencoder using the training data (x_train) and aims to reconstruct the same data as the target.
epochs=20 specifies the number of training iterations.
batch_size=256 defines the number of samples per gradient update.
shuffle=True shuffles the training data before each epoch.
validation_data=(x_test, x_test) provides the test data to evaluate the model's performance on unseen data during training.
#### Gets Reconstructed Images:
The trained autoencoder is used to predict (reconstruct) the test images using autoencoder.predict(x_test).
#### Defines a Plotting Function:
The plot_images() function takes original and reconstructed images as input and displays the first n (default 10) original images in the top row and their corresponding reconstructed images in the bottom row using matplotlib.
#### Plots Original vs. Reconstructed Images:
The plot_images() function is called with the test images (x_test) and the reconstructed images (reconstructed_imgs) to visually compare the original and the autoencoder's output.
### How to Run the Code
Because we are doing it in Google colab we donot need to worry about the neccessary packages.
The script will:

Download the MNIST dataset (if not already downloaded).
Train the autoencoder model. You will see the training progress with loss and validation loss for each epoch.
After training, it will generate a Matplotlib plot showing the first 10 original test images and their corresponding reconstructed images. Close the plot window to end the script.

### Expected Output
The script will output the training progress in the console, showing the loss and validation loss decreasing over the 20 epochs. Additionally, a Matplotlib window will display a figure with two rows:

The top row will show the first 10 original handwritten digit images from the test set.
The bottom row will show the corresponding reconstructed digit images generated by the autoencoder.
You should observe that the reconstructed images resemble the original digits, although they might be slightly blurred or less sharp due to the information compression in the latent space.

### Further Exploration
#### Vary latent_dim
Experiment with different values for latent_dim (e.g., 16, 64). A smaller latent_dim will force more aggressive compression, potentially leading to more information loss and blurrier reconstructions. A larger latent_dim might result in better reconstructions but less effective dimensionality reduction.

#### Add More Layers: 
Explore adding more layers to both the encoder and the decoder to create a deeper autoencoder.
#### Use Convolutional Layers:
For image data like MNIST, convolutional autoencoders (using Conv2D and Conv2DTranspose layers) often perform better at capturing spatial relationships in the images.
#### Experiment with Activation Functions:
Try different activation functions in the encoder and decoder layers.
#### Change the Loss Function:
For pixel values in the range [0, 1], binary_crossentropy is a common choice. For pixel values in other ranges, mean squared error (mse) might be more appropriate.
#### Visualize the Latent Space:
After training, you can use the encoder part of the autoencoder to map the test images to the 32-dimensional latent space. If the latent_dim were 2 or 3, you could visualize this latent space and see if digits of the same class cluster together. For higher dimensions, dimensionality reduction techniques like t-SNE or PCA can be used for visualization.

## Question 2
### Code Description

#### Imports Libraries: 
Imports necessary libraries: numpy for numerical operations, matplotlib.pyplot for plotting, and tensorflow and its Keras API for building and training the neural network.
#### Loads MNIST Dataset:
Loads the MNIST dataset using tensorflow.keras.datasets.mnist.load_data(). The dataset is split into training and testing sets.
#### Preprocesses Data:
Converts the pixel values to floating-point numbers and normalizes them to the range [0, 1] by dividing by 255.0.
Flattens the 28x28 pixel images into vectors of 784 elements using reshape().
#### Adds Gaussian Noise:
Defines a function add_noise() that takes a batch of images and a noise_factor as input.
It adds random Gaussian noise (with mean 0.0 and standard deviation controlled by noise_factor) to the images.
The noisy pixel values are clipped to the range [0, 1].
Applies the add_noise() function to both the training and testing datasets to create x_train_noisy and x_test_noisy.
#### Defines the Denoising Autoencoder Model:
##### Encoder:
An Input layer is defined with a shape of (784,), representing the flattened noisy input image.
A Dense layer with latent_dim (set to 32) units and ReLU activation is used to encode the noisy input into the latent space.
##### Decoder:
A Dense layer with 784 units (the original image dimension) and sigmoid activation is used to decode the latent representation back to the (hopefully) clean image space. Sigmoid activation ensures the output pixel values are between 0 and 1.
##### Autoencoder Model:
A Model is created that takes the input_img and outputs the decoded image.
##### Compilation:
The autoencoder model is compiled using the adam optimizer and binary_crossentropy loss function. The goal is to minimize the difference between the denoised output and the original clean input.
#### Trains the Model:
The fit() method trains the denoising autoencoder. It takes the noisy training data (x_train_noisy) as input and the original clean training data (x_train) as the target to reconstruct.
epochs=20 specifies the number of training iterations.
batch_size=256 defines the number of samples per gradient update.
shuffle=True shuffles the training data before each epoch.
validation_data=(x_test_noisy, x_test) provides the noisy test data as input and the clean test data as the target to evaluate the model's denoising performance on unseen data during training.
#### Gets Reconstructed Images:
The trained autoencoder is used to predict (reconstruct, i.e., denoise) the noisy test images using autoencoder.predict(x_test_noisy).
#### Defines a Plotting Function:
The plot_images() function takes noisy and reconstructed images as input and displays the first n (default 10) noisy images in the top row and their corresponding reconstructed (denoised) images in the bottom row using matplotlib.
#### Plots Noisy vs. Reconstructed Images:
The plot_images() function is called with the noisy test images (x_test_noisy) and the reconstructed images (reconstructed_imgs) to visually compare the noisy inputs and the autoencoder's denoised output.
### How to Run the Code
Because we are coding in Google colab we donot need to worry about the required packages as they are automatically installed
#### The script will:

Download the MNIST dataset (if not already downloaded).
Add Gaussian noise to the training and testing images.
Train the denoising autoencoder model. You will see the training progress with loss and validation loss for each epoch.
After training, it will generate a Matplotlib plot showing the first 10 noisy test images and their corresponding denoised (reconstructed) images. Close the plot window to end the script.
### Expected Output
The script will output the training progress in the console, showing the loss and validation loss decreasing over the 20 epochs. Additionally, a Matplotlib window will display a figure with two rows:

The top row will show the first 10 noisy handwritten digit images from the test set.
The bottom row will show the corresponding denoised (reconstructed) digit images generated by the autoencoder.
You should observe that the reconstructed images are less noisy and more closely resemble the original clean digits compared to the noisy input. The effectiveness of the denoising will depend on the noise_factor and the capacity of the autoencoder.

### Further Exploration
#### Vary latent_dim:
Experiment with different values for latent_dim (e.g., 16, 64). A smaller latent_dim might force the autoencoder to learn more robust features for denoising, while a larger latent_dim might allow it to capture more details but could also be more susceptible to learning the noise.
#### Adjust noise_factor:
Change the noise_factor in the add_noise() function to control the level of noise added to the images. Observe how the denoising performance changes with different noise levels.
#### Add More Layers:
Explore adding more layers to both the encoder and the decoder to create a deeper denoising autoencoder. This might improve the model's ability to learn complex features and denoise more effectively.

## Question 3
### Code Description

#### Imports Libraries:
Imports necessary libraries: numpy for numerical operations, tensorflow and its Keras API for building and training the neural network, random for random choices, sys for system-specific parameters and functions, and re for regular expressions (though re is not used in the provided snippet).
#### Loads Dataset:
Downloads the Shakespeare Sonnets text file from a GitHub repository using tf.keras.utils.get_file().
Reads the content of the file and converts it to lowercase.
#### Prepares Character Mapping:
Creates a sorted list of unique characters present in the text.
Creates two dictionaries: char_to_idx to map each character to a numerical index, and idx_to_char to map each index back to its corresponding character.
#### Creates Sequences:
Defines seq_length (the length of input sequences) and step (the interval between the start of consecutive sequences).
Creates lists sentences (containing input sequences) and next_chars (containing the character that follows each sequence).
Converts the sentences and next_chars into NumPy arrays suitable for training the model. X is a 3D array where each element X[i, t, char_to_idx[char]] is 1 if the character at position t in the i-th sentence is char, and 0 otherwise (one-hot encoding). y is a 2D array where each element y[i, char_to_idx[next_chars[i]]] is 1 if the next character for the i-th sentence is next_chars[i], and 0 otherwise (one-hot encoding).
#### Defines RNN Model:
Creates a sequential model using tensorflow.keras.models.Sequential.
Adds an Input layer specifying the shape of the input sequences: (seq_length, len(chars)).
Adds an LSTM layer with 128 units. LSTM layers are well-suited for capturing long-range dependencies in sequential data.
Adds a Dense output layer with len(chars) units (one for each possible character) and a softmax activation function. Softmax outputs a probability distribution over the possible next characters.
Compiles the model using the categorical_crossentropy loss function (suitable for multi-class classification where the classes are one-hot encoded characters) and the adam optimizer.
#### Trains Model:
Trains the defined model using the prepared input data X and target data y.
batch_size=128 specifies the number of sequences processed in each batch during training.
epochs=10 specifies the number of times the entire training dataset is passed through the model.
#### Defines Text Generation Function:
Defines a function generate_text() that takes a seed_text (the starting sequence), the desired length of the generated text, and a temperature parameter to control the randomness of the generated text.
##### Inside the function:
It initializes the generated text with the seed_text.
It iterates length times to generate new characters.
In each iteration, it one-hot encodes the current seed_text to create the input x_pred for the model.
It uses model.predict() to get the probability distribution over the next possible characters.
The temperature parameter is used to adjust the probability distribution. A lower temperature makes the model more confident in its most likely predictions (less random), while a higher temperature makes the predictions more diverse and random.
It samples the next character based on the adjusted probability distribution using np.random.choice().
The generated character is appended to the generated text, and the seed_text is updated by removing the first character and adding the newly generated character (sliding window).
The function returns the complete generated text.
#### Example Text Generation:
Sets an example seed text.
Calls the generate_text() function with the seed, a default length of 200, and a default temperature of 1.0.
Prints the generated text.
#### How to Run the Code
Because we code in Google colab we donot need to worry about installing required packages as they are already installed
#### The script will:

Download the Shakespeare Sonnets dataset (if not already downloaded).
Preprocess the text and create training sequences.
Define and train the LSTM model. You will see the training progress with loss for each epoch.
After training, it will use the provided seed text to generate new text and print the generated output to the console.
### Expected Output
The script will first show the training progress of the LSTM model, with the loss value decreasing over the 10 epochs. After training is complete, it will print a block of generated text that starts with the provided seed ("shall i compare thee to a summer's day? "). The generated text will attempt to mimic the style and character patterns present in the Shakespeare Sonnets dataset. The quality and coherence of the generated text will depend on the size and complexity of the training data, the model architecture, and the number of training epochs.

### Further Exploration
##### Train for More Epochs:
Training the model for a larger number of epochs can often lead to better learning of the underlying patterns in the text and improved generation quality.
##### Increase LSTM Units:
Increasing the number of units in the LSTM layer (e.g., to 256 or 512) can increase the model's capacity to learn more complex relationships.
##### Add More LSTM Layers:
Stacking multiple LSTM layers can help the model learn hierarchical representations of the text.
##### Experiment with seq_length and step:
Changing the length of the input sequences and the step size can affect what the model learns and how it generates text.
##### Adjust temperature:
Experiment with different values of the temperature parameter in the generate_text() function to control the randomness and creativity of the generated text. Values closer to 0 will produce more predictable text, while values greater than 1 will introduce more randomness.
##### Train on Different Datasets:
Try training the model on different text datasets to generate text in various styles (e.g., news articles, code, etc.).
##### Implement Character-Level Embeddings:
Instead of one-hot encoding the characters, you could use an embedding layer to learn vector representations for each character, which can sometimes improve performance.
#### Use Convolutional Layers:
For image data, convolutional denoising autoencoders (using Conv2D, MaxPooling2D, Conv2DTranspose, and UpSampling2D layers) are often more effective at capturing spatial dependencies and removing noise.
#### Experiment with Activation Functions:
Try different activation functions in the encoder and decoder layers.
#### Change the Loss Function:
While binary_crossentropy is suitable for pixel values in the range [0, 1], mean squared error (mse) is another common choice for denoising tasks.

## Question 4
### Code Description
#### Imports Libraries:
Imports necessary libraries: numpy for numerical operations, tensorflow and its Keras API for building and training the neural network, Embedding, SpatialDropout1D, LSTM, Dense, Tokenizer, pad_sequences for text processing, imdb dataset from Keras, confusion_matrix and classification_report from scikit-learn for evaluation, and matplotlib.pyplot and seaborn for plotting.
#### Loads IMDB Dataset:
Loads the IMDB dataset using imdb.load_data(num_words=max_words). num_words=max_words limits the vocabulary size to the top max_words most frequent words.
The dataset is split into training (x_train, y_train) and testing (x_test, y_test) sets. x_train and x_test contain sequences of word indices, and y_train and y_test contain the corresponding sentiment labels (0 for negative, 1 for positive).
#### Pads Sequences:
Uses pad_sequences() to ensure that all input sequences have the same length (max_len). Sequences shorter than max_len are padded with zeros at the beginning, and sequences longer than max_len are truncated. This is necessary because LSTM layers expect inputs of consistent shape.
#### Defines LSTM Model:
Creates a sequential model using tensorflow.keras.models.Sequential.
Adds an Embedding layer: This layer converts the integer word indices into dense vector representations of size 128. The embedding layer is learned during training. input_dim is max_words (the vocabulary size), and output_dim is 128 (the dimensionality of the embedding vectors).
Adds a SpatialDropout1D layer with a dropout rate of 0.2. This layer applies dropout to entire 1D feature maps, which can help prevent overfitting in sequence models.
Adds an LSTM layer with 100 units. dropout=0.2 and recurrent_dropout=0.2 apply dropout to the input and recurrent connections of the LSTM, respectively, further mitigating overfitting.
Adds a Dense output layer with 1 unit and a sigmoid activation function. Sigmoid activation is used for binary classification, outputting a value between 0 and 1 representing the probability of the review being positive.
Compiles the model using the binary_crossentropy loss function (suitable for binary classification with sigmoid output), the adam optimizer, and accuracy as the evaluation metric.
#### Trains Model:
Trains the defined model using the preprocessed training data x_train and y_train.
epochs=5 specifies the number of times the entire training dataset is passed through the model.
batch_size=64 defines the number of samples processed in each batch during training.
validation_data=(x_test, y_test) provides the test data to evaluate the model's performance on unseen data during training.
#### Evaluates Model:
Uses the trained model to predict the sentiment of the test reviews using model.predict(x_test). The output is a probability between 0 and 1.
Converts the probabilities to binary predictions (0 or 1) by applying a threshold of 0.5.
Calculates the confusion matrix using confusion_matrix(y_test, y_pred), which shows the counts of true positives, true negatives, false positives, and false negatives.
Generates a classification report using classification_report(y_test, y_pred, target_names=['Negative', 'Positive']), which provides precision, recall, F1-score, and support for each class.
#### Plots Confusion Matrix:
Creates a heatmap of the confusion matrix using seaborn.heatmap().
annot=True displays the numerical values in the heatmap cells.
fmt='d' formats the annotations as integers.
cmap='Blues' sets the color map.
xticklabels and yticklabels label the axes with the class names.
Sets the labels for the x-axis (Predicted), y-axis (Actual), and the title of the plot.
Displays the plot using plt.show().
Prints the classification report to the console.
#### How to Run the Code
Because we code in Google colab we donot need to worry about installing required packages as they are already installed

##### The script will:

Download the IMDB dataset (if not already downloaded).
Preprocess the text data by padding sequences.
Define and train the LSTM model. You will see the training progress with accuracy and loss on both the training and validation sets for each epoch.
After training, it will evaluate the model on the test set, calculate the confusion matrix and classification report.
A Matplotlib window will display the confusion matrix as a heatmap.
The classification report will be printed to the console.
#### Expected Output
The script will first show the training progress of the LSTM model, with accuracy generally increasing and loss decreasing over the 5 epochs. After training, a Matplotlib window will display a confusion matrix visualizing the model's predictions on the test set. The console will also output a classification report containing precision, recall, F1-score, and support for both the 'Negative' and 'Positive' sentiment classes, as well as the overall accuracy. The specific values will vary slightly depending on the random initialization of the model and the training process.

#### Further Exploration
##### Increase max_words and max_len:
Experiment with larger vocabulary sizes and maximum review lengths to see if it improves performance. However, this will also increase the computational cost.
##### Increase Embedding Dimension:
Try using a larger embedding dimension (e.g., 256) to capture more semantic information.
##### Add More LSTM Layers:
Stacking multiple LSTM layers can help the model learn more complex patterns in the text.
##### Experiment with Dropout Rates:
Adjust the dropout rates in the SpatialDropout1D and LSTM layers to potentially improve generalization.
##### Use Pre-trained Word Embeddings:
Instead of learning embeddings from scratch, you could use pre-trained word embeddings like Word2Vec, GloVe, or FastText, which have been trained on large corpora and can provide better initial representations of words.
##### Try Different Optimizers:
Experiment with other optimizers like RMSprop or AdamW.
##### Increase Training Epochs:
Training for more epochs might lead to better performance, but you should also monitor the validation loss to avoid overfitting.
##### Implement Bidirectional LSTM:
Using a Bidirectional wrapper around the LSTM layer can allow the model to consider the context of words from both preceding and succeeding parts of the sequence, which can often improve performance in NLP tasks.
